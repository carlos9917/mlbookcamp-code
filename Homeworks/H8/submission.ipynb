{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f9405b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fe358e-2664-40dc-82e1-e60ea87474f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 11:56:45.615688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-19 11:56:45.615711: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import Xception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660562-263b-490a-9e0a-5d39d17d11dd",
   "metadata": {},
   "source": [
    "# Setting up the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0a2a8d-1d95-40cc-8fdc-ef5ebcd65750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), \n",
    "                 activation ='relu', input_shape = (150,150,3)))\n",
    "                 \n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation = \"relu\"))\n",
    "    model.add(Dense(1, activation='sigmoid')) #since is binary classification\n",
    "    optimizer = SGD(lr=0.002, momentum=0.8)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb14c3e7-ba76-40ec-ac9e-386589e7cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 175232)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,215,873\n",
      "Trainable params: 11,215,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 11:56:50.886816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-19 11:56:50.886842: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-19 11:56:50.886866: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9h8lvp2.usr.local): /proc/driver/nvidia/version does not exist\n",
      "2021-11-19 11:56:50.887102: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/media/cap/7fed51bd-a88e-4971-9656-d617655b6312/data/sources/conda/miniconda3/envs/py39/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = define_model()\n",
    "\n",
    "# Looking for the number of total parameters\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89bfdfd0-bc11-407a-a61e-233c4cc80647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answer to Q1 is 11,215,873"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b37897-d40a-4f86-9f79-bc9f13726d5f",
   "metadata": {},
   "source": [
    "# Now preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59189590-2c6d-4c80-aa70-8adea67b3511",
   "metadata": {},
   "source": [
    "## For Q2 we are told to do the following\n",
    "\n",
    "Generators and Training\n",
    "For the next two questions, use the following data generator for both train and validation:\n",
    "\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "We don't need to do any additional pre-processing for the images.\n",
    "When reading the data from train/val directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "Use batch_size=20\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba0bd0f-c538-49f4-8938-7c2c8e3b4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "DATAPATH=\"/media/cap/7fed51bd-a88e-4971-9656-d617655b6312/data/ML_Data/BootCamp/\"\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "#not sure I need to do it twice\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(os.path.join(DATAPATH,\"train\"),\n",
    "class_mode='binary', batch_size=20, target_size=(150, 150))\n",
    "\n",
    "validation_ds = val_gen.flow_from_directory(os.path.join(DATAPATH,\"validation\"),\n",
    " class_mode='binary', batch_size=20, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431e1579-7d91-410d-9803-ef489a428020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# train_ds = train_gen.flow_from_directory(\n",
    "#     os.path.join(DATAPATH,\"train\"),\n",
    "#     target_size=(150, 150),\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6195530a-9e9a-43b7-96b4-a01f83f2e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGD(lr=0.002,momentum=0.8)\n",
    "#model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9708ed8-87b9-439d-ac48-df84a42d33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 22s 219ms/step - loss: 0.5510 - accuracy: 0.7305 - val_loss: 0.5872 - val_accuracy: 0.6890\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 21s 210ms/step - loss: 0.5303 - accuracy: 0.7435 - val_loss: 0.5649 - val_accuracy: 0.7110\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 19s 193ms/step - loss: 0.5400 - accuracy: 0.7315 - val_loss: 0.5711 - val_accuracy: 0.7150\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 0.5295 - accuracy: 0.7450 - val_loss: 0.5517 - val_accuracy: 0.7140\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 21s 213ms/step - loss: 0.5184 - accuracy: 0.7465 - val_loss: 0.5772 - val_accuracy: 0.6960\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 0.5197 - accuracy: 0.7355 - val_loss: 0.5743 - val_accuracy: 0.7130\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 0.4885 - accuracy: 0.7655 - val_loss: 0.5499 - val_accuracy: 0.7140\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.5017 - accuracy: 0.7585 - val_loss: 0.5394 - val_accuracy: 0.7320\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.4864 - accuracy: 0.7735 - val_loss: 0.5639 - val_accuracy: 0.7100\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 0.4716 - accuracy: 0.7760 - val_loss: 0.5438 - val_accuracy: 0.7220\n"
     ]
    }
   ],
   "source": [
    "history_callback=model.fit( train_ds, steps_per_epoch=100, epochs=10, validation_data=validation_ds, validation_steps=50 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3863e492-d690-4c68-8f64-b1d469391944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median training acc: 0.7457500100135803\n",
      "std of training loss: 0.024408338916025778\n",
      "mean of validation loss: 0.5623502135276794\n",
      "avg val acc for epochs 5 to 10: 0.7144999901453654\n"
     ]
    }
   ],
   "source": [
    "median_acc = np.median(history_callback.history[\"accuracy\"])\n",
    "sd_loss = np.std(history_callback.history[\"loss\"])\n",
    "mean_vloss = np.mean(history_callback.history[\"val_loss\"])\n",
    "avg_val_acc_5_10 = np.mean(history_callback.history[\"val_accuracy\"][4:])\n",
    "print(f\"median training acc: {median_acc}\")\n",
    "print(f\"std of training loss: {sd_loss}\")\n",
    "print(f\"mean of validation loss: {mean_vloss}\")\n",
    "print(f\"avg val acc for epochs 5 to 10: {avg_val_acc_5_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dd57d-935e-477d-957f-b653eff25fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105299e-036a-4c58-9e0a-d500380e77ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "905fd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data agumentation part\n",
    "# We are not supposed to \"re-create the model - we want to continue training the model we already started training\"\n",
    "# Apparently this means:\n",
    "# Yes you don't need to recompile it. But even if you compile again, it doesn't reset the model you trained previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21bddd43-b4fa-4289-bb74-93f0ecc59a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "train_ds = train_gen.flow_from_directory(os.path.join(DATAPATH,\"train\"),\n",
    "class_mode='binary', batch_size=20, target_size=(150, 150))\n",
    "\n",
    "validation_ds = train_gen.flow_from_directory(os.path.join(DATAPATH,\"validation\"),\n",
    " class_mode='binary', batch_size=20, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621aa338-70fd-48f9-a0ed-c6c621c38f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add_metric(\n",
    "#            keras.backend.std(inputs), name=\"std_of_activation\", aggregation=\"mean\"\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e71daf26-61e0-4b0f-adcf-a730759bdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 0.6143 - accuracy: 0.6645 - val_loss: 0.6308 - val_accuracy: 0.6390\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 0.6244 - accuracy: 0.6600 - val_loss: 0.5981 - val_accuracy: 0.6920\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.6209 - accuracy: 0.6510 - val_loss: 0.6107 - val_accuracy: 0.6600\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 0.6153 - accuracy: 0.6570 - val_loss: 0.5939 - val_accuracy: 0.6910\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 28s 279ms/step - loss: 0.6082 - accuracy: 0.6650 - val_loss: 0.5985 - val_accuracy: 0.6740\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6104 - accuracy: 0.6685 - val_loss: 0.5954 - val_accuracy: 0.6660\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 0.6078 - accuracy: 0.6650 - val_loss: 0.6171 - val_accuracy: 0.6530\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.5945 - accuracy: 0.6705 - val_loss: 0.5828 - val_accuracy: 0.7010\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.6106 - accuracy: 0.6665 - val_loss: 0.5996 - val_accuracy: 0.6770\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.6026 - accuracy: 0.6695 - val_loss: 0.5993 - val_accuracy: 0.6900\n"
     ]
    }
   ],
   "source": [
    "history_callback = model.fit( train_ds, steps_per_epoch=100, epochs=10, validation_data=validation_ds, validation_steps=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3690136-f613-44a4-b6de-56529ab31f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median training acc: 0.6650000214576721\n",
      "std of training loss: 0.008172949186204842\n",
      "mean of validation loss: 0.6026202142238617\n",
      "avg val acc for epochs 5 to 10: 0.6768333315849304\n"
     ]
    }
   ],
   "source": [
    "median_acc = np.median(history_callback.history[\"accuracy\"])\n",
    "sd_loss = np.std(history_callback.history[\"loss\"])\n",
    "mean_vloss = np.mean(history_callback.history[\"val_loss\"])\n",
    "avg_val_acc_5_10 = np.mean(history_callback.history[\"val_accuracy\"][4:])\n",
    "print(f\"median training acc: {median_acc}\")\n",
    "print(f\"std of training loss: {sd_loss}\")\n",
    "print(f\"mean of validation loss: {mean_vloss}\")\n",
    "print(f\"avg val acc for epochs 5 to 10: {avg_val_acc_5_10}\")\n",
    "#print(history_callback.history[\"val_accuracy\"])\n",
    "\n",
    "#print(history_callback.history[\"val_accuracy\"][4:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e885ab8-9393-4a53-bc12-b65c8f0f8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 35s 139ms/step - loss: 0.6436 - accuracy: 0.6284\n",
      "0.6284000277519226\n"
     ]
    }
   ],
   "source": [
    "#scores = model.evaluate(validation_ds)\n",
    "#print(scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263dbd8",
   "metadata": {},
   "source": [
    "## 8.13 Summary\n",
    "\n",
    "* We can use pre-trained models for general image classification\n",
    "* Convolutional layers let us turn an image into a vector\n",
    "* Dense layers use the vector to make the predictions\n",
    "* Instead of training a model from scratch, we can use transfer learning and re-use already trained convolutional layers\n",
    "* First, train a small model (150x150) before training a big one (299x299)\n",
    "* Learning rate - how fast the model trians. Fast learners aren't always best ones\n",
    "* We can save the best model using callbacks and checkpointing\n",
    "* To avoid overfitting, use dropout and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5378439",
   "metadata": {},
   "source": [
    "## 8.14 Explore more\n",
    "\n",
    "* Add more data, e.g. Zalando, etc (ADD LINKS)\n",
    "* Albumentations - another way of generating augmentations\n",
    "* Use PyTorch or MXNet instead of TensorFlow/Keras\n",
    "* In addition to Xception, there are others architectures - try them \n",
    "\n",
    "Other projects:\n",
    "\n",
    "* cats vs dogs\n",
    "* Hotdog vs not hotdog\n",
    "* Category of images\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
